# Scraper d'Emails et RÃ©seaux Sociaux avec SystÃ¨me de Queue

Scraper web asynchrone en Python pour extraire les emails et rÃ©seaux sociaux de sites internet. SystÃ¨me de queue intÃ©grÃ© pour gÃ©rer plusieurs requÃªtes.

## ğŸ¯ FonctionnalitÃ©s

- âœ… Extraction d'emails avec filtrage intelligent (domaine du site + fournisseurs connus)
- âœ… Extraction de rÃ©seaux sociaux (Facebook, Instagram, Twitter, LinkedIn, etc.)
- âœ… Scraping multi-pages (contact, CGV, mentions lÃ©gales, etc.)
- âœ… **SystÃ¨me de queue FIFO** pour gÃ©rer plusieurs requÃªtes
- âœ… **Gestion des prioritÃ©s** (1=haute, 10=basse)
- âœ… Scraping asynchrone optimisÃ© (~2.5s par site)
- âœ… Retry logic et rotation User-Agents
- âœ… Rapports JSON simplifiÃ©s

## ğŸ“‹ PrÃ©requis

- Python 3.7 ou supÃ©rieur
- pip (gestionnaire de paquets Python)

## ğŸš€ Installation

```bash
pip install -r requirements.txt
```

## ğŸ“– Utilisation

### Ã‰TAPE 1 : DÃ©marrer le worker (Ã  laisser tourner)

```bash
python worker.py
```

Le worker attend des jobs dans la queue et les traite automatiquement.

### Ã‰TAPE 2 : Ajouter des jobs Ã  la queue

**Terminal 1 (User 1) :**
```bash
python add_job.py google-maps-avocats.csv --priority 1
```

**Terminal 2 (User 2) :**
```bash
python add_job.py google-maps-pharmacies.csv --priority 2
```

**Terminal 3 (User 3) :**
```bash
python add_job.py google-maps-notaires.csv --priority 3
```

Les jobs seront traitÃ©s dans l'ordre de prioritÃ© (1 Ã  10), puis par ordre d'arrivÃ©e.

### Ã‰TAPE 3 : Consulter l'Ã©tat de la queue

```bash
python monitor.py
```

Affiche :
- Nombre de jobs en attente / en traitement / terminÃ©s
- Job en cours avec progression
- Derniers jobs complÃ©tÃ©s avec stats

## ğŸ“Š RÃ©sultats

Les rÃ©sultats sont sauvegardÃ©s dans `results/` avec un format simplifiÃ© :

**Fichier :** `scraping_nom-du-csv_YYYYMMDD_HHMMSS.json`

### Structure simplifiÃ©e

```json
[
  {
    "id": 1,
    "url": "https://example.com",
    "nom": "Example Site",
    "nb_emails": 2,
    "emails": [
      "contact@example.com",
      "info@example.com"
    ],
    "nb_reseaux_sociaux": 3,
    "reseaux_sociaux": {
      "facebook": ["https://facebook.com/example"],
      "instagram": ["https://instagram.com/example"],
      "linkedin": ["https://linkedin.com/company/example"]
    }
  }
]
```

## ğŸ”§ Configuration

ParamÃ¨tres optimisÃ©s dans `config.py` :

- **MAX_CONCURRENT_SITES** : 10 sites en parallÃ¨le
- **MAX_PAGES_PER_SITE** : 7 pages par site
- **TIMEOUT** : 10 secondes par requÃªte
- **SITE_TIMEOUT** : 30 secondes par site
- **DELAY_BETWEEN_REQUESTS** : 0.3 secondes (optimisÃ©)

**Performance moyenne : ~2.5s par site**

## ğŸ“§ Filtrage des Emails

Le scraper garde uniquement les emails qui :

1. **Appartiennent au domaine du site** (ex: contact@example.com pour example.com)
2. **Proviennent de fournisseurs connus** (gmail, hotmail, yahoo, outlook, etc.)

Cela Ã©vite les faux positifs et les emails non pertinents.

## ğŸŒ Pages ScrapÃ©es

Le scraper visite automatiquement :

- Page d'accueil
- /contact, /contactez-nous
- /mentions-legales, /legal-notice
- /cgv, /conditions-generales-vente
- /cgu, /conditions-generales-utilisation
- /politique-confidentialite, /privacy-policy
- /rgpd, /donnees-personnelles
- /about, /a-propos
- Liens dÃ©tectÃ©s dans le footer

## ğŸ” Sections AnalysÃ©es

Pour chaque page, le scraper analyse :

- **Footer** (zone la plus riche en informations de contact)
- **Header/Navigation**
- **Sections Contact**
- **Mentions lÃ©gales / CGV / CGU**
- **Sidebar**
- **MÃ©tadonnÃ©es** (balises meta)
- **DonnÃ©es structurÃ©es** (JSON-LD, Schema.org)
- **Liens mailto:**

## ğŸ“ Logs

Les logs sont sauvegardÃ©s dans `scraper.log` avec :
- Progression du scraping
- Emails et rÃ©seaux sociaux trouvÃ©s
- Erreurs et timeouts
- Statistiques finales

## ğŸ”„ SystÃ¨me de Queue

### Workflow multi-utilisateurs :

1. **User 1** ajoute un job â†’ `add_job.py avocats.csv --priority 1`
2. **User 2** ajoute un job â†’ `add_job.py pharmacies.csv --priority 2`  
3. **Worker** traite les jobs dans l'ordre (prioritÃ© puis FIFO)
4. **RÃ©sultats** disponibles dans `results/`

### Commandes :

```bash
# DÃ©marrer le worker (terminal dÃ©diÃ©)
python worker.py

# Ajouter des jobs (n'importe quel terminal)
python add_job.py mon_fichier.csv --priority 5 --user "Nom"

# Voir l'Ã©tat de la queue
python monitor.py
```

## âš ï¸ SÃ©curitÃ© et Bonnes Pratiques

- âœ… **1 seul worker** â†’ Ã‰vite bannissement IP
- âœ… **DÃ©lais entre requÃªtes** (0.3s) â†’ Respectueux
- âœ… **Filtrage emails** â†’ Seulement domaine du site + fournisseurs connus
- âš ï¸ **Usage lÃ©gal uniquement** â†’ Sites publics seulement
- âš ï¸ **Respecter RGPD** â†’ Ne pas abuser des donnÃ©es

## ğŸ› DÃ©pannage

### Le worker ne traite pas les jobs

VÃ©rifiez que le worker tourne : `python worker.py`

### Erreur "CSV introuvable"

Utilisez le chemin complet ou relatif correct du CSV

## ğŸ“„ Licence

Usage Ã©ducatif et personnel uniquement.


