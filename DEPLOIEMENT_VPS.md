# Guide de DÃ©ploiement sur VPS Hostinger

## ğŸ“‹ PrÃ©requis

- VPS Hostinger avec Ubuntu 20.04+ ou Debian 11+
- AccÃ¨s SSH root ou sudo
- Au moins 1 GB RAM (2 GB recommandÃ©)
- 10 GB d'espace disque

## ğŸš€ Installation sur VPS

### Ã‰tape 1 : Connexion SSH au VPS

```bash
ssh root@votre-ip-vps
# ou
ssh user@votre-ip-vps
```

### Ã‰tape 2 : Cloner le projet depuis GitHub

```bash
cd /home/votre-user
git clone https://github.com/votre-username/scrapeur-site-web.git
cd scrapeur-site-web
```

### Ã‰tape 3 : Installation automatique

```bash
bash install_vps.sh
```

Ce script va :
- Mettre Ã  jour le systÃ¨me
- Installer Python 3 et pip
- CrÃ©er un environnement virtuel
- Installer toutes les dÃ©pendances
- CrÃ©er les dossiers nÃ©cessaires

### Ã‰tape 4 : Installer le service systemd (worker permanent)

```bash
sudo bash install_service.sh
```

### Ã‰tape 5 : DÃ©marrer le worker

```bash
# DÃ©marrer le service
sudo systemctl start scraper-worker

# Activer au dÃ©marrage du VPS
sudo systemctl enable scraper-worker

# VÃ©rifier le status
sudo systemctl status scraper-worker
```

## ğŸ“Š Utilisation sur VPS

### Ajouter des jobs via SSH

**Option 1 : SSH direct**
```bash
ssh user@votre-ip-vps
cd /home/user/scrapeur-site-web
source venv/bin/activate
python add_job.py mon_fichier.csv --priority 1 --user "Alice"
```

**Option 2 : Upload CSV + ajouter job**
```bash
# Depuis votre PC local
scp mon_fichier.csv user@votre-ip-vps:/home/user/scrapeur-site-web/

# Puis SSH
ssh user@votre-ip-vps
cd /home/user/scrapeur-site-web
source venv/bin/activate
python add_job.py mon_fichier.csv --priority 1
```

### Consulter l'Ã©tat de la queue

```bash
ssh user@votre-ip-vps
cd /home/user/scrapeur-site-web
source venv/bin/activate
python monitor.py
```

### RÃ©cupÃ©rer les rÃ©sultats

```bash
# Depuis votre PC local
scp user@votre-ip-vps:/home/user/scrapeur-site-web/results/*.json ./
```

## ğŸ”§ Gestion du Service

### Commandes principales

```bash
# DÃ©marrer le worker
sudo systemctl start scraper-worker

# ArrÃªter le worker
sudo systemctl stop scraper-worker

# RedÃ©marrer le worker
sudo systemctl restart scraper-worker

# Voir le status
sudo systemctl status scraper-worker

# Activer au dÃ©marrage
sudo systemctl enable scraper-worker

# DÃ©sactiver au dÃ©marrage
sudo systemctl disable scraper-worker
```

### Consulter les logs

```bash
# Logs du worker
tail -f logs/worker.log

# Logs d'erreur
tail -f logs/worker_error.log

# Logs systemd
sudo journalctl -u scraper-worker -f
```

## ğŸŒ API Web (Optionnel - Future amÃ©lioration)

Pour faciliter l'utilisation, vous pouvez ajouter une API web :

**Structure :**
```
api/
  â””â”€â”€ app.py          # Flask/FastAPI
      â”œâ”€â”€ POST /upload    # Upload CSV
      â”œâ”€â”€ POST /job       # CrÃ©er job
      â”œâ”€â”€ GET /status     # Ã‰tat de la queue
      â””â”€â”€ GET /results/:id # TÃ©lÃ©charger rÃ©sultat
```

**Installation :**
```bash
pip install flask flask-cors
python api/app.py
```

**AccÃ¨s :**
- API : `http://votre-ip-vps:5000`
- Upload CSV via interface web
- TÃ©lÃ©chargement automatique des rÃ©sultats

## ğŸ”’ SÃ©curitÃ© VPS

### Firewall

```bash
# Autoriser seulement SSH
sudo ufw allow 22/tcp
sudo ufw enable

# Si API Web activÃ©e
sudo ufw allow 5000/tcp
```

### CrÃ©er un utilisateur dÃ©diÃ©

```bash
# CrÃ©er utilisateur scraper
sudo adduser scraper
sudo usermod -aG sudo scraper

# Installer le projet dans /home/scraper
su - scraper
```

### Limiter les ressources

Modifier `/etc/systemd/system/scraper-worker.service` :

```ini
[Service]
# Limiter RAM Ã  2 GB
MemoryLimit=2G

# Limiter CPU Ã  50%
CPUQuota=50%
```

## ğŸ“ˆ Monitoring et Maintenance

### Cron job pour nettoyage

CrÃ©er un cron pour nettoyer les vieux rÃ©sultats :

```bash
crontab -e
```

Ajouter :
```bash
# Nettoyer rÃ©sultats de plus de 30 jours Ã  3h du matin
0 3 * * * find /home/user/scrapeur-site-web/results -name "*.json" -mtime +30 -delete
0 3 * * * find /home/user/scrapeur-site-web/queue/completed -name "*.json" -mtime +30 -delete
```

### Rotation des logs

CrÃ©er `/etc/logrotate.d/scraper` :

```
/home/user/scrapeur-site-web/logs/*.log {
    daily
    rotate 7
    compress
    delaycompress
    missingok
    notifempty
}
```

## ğŸ”„ Mise Ã  jour du code

```bash
cd /home/user/scrapeur-site-web
git pull origin main
source venv/bin/activate
pip install -r requirements.txt
sudo systemctl restart scraper-worker
```

## âš¡ Optimisation VPS

### Pour VPS avec peu de RAM (1 GB)

Modifier `config.py` :
```python
MAX_CONCURRENT_SITES = 5  # RÃ©duire de 10 Ã  5
```

### Pour VPS puissant (4+ GB RAM)

```python
MAX_CONCURRENT_SITES = 15  # Augmenter Ã  15
```

## ğŸ“Š Estimation des Ressources

**VPS RecommandÃ© :**
- **CPU** : 1-2 vCores
- **RAM** : 2 GB minimum
- **Disque** : 20 GB
- **Bande passante** : IllimitÃ©e recommandÃ©e

**Consommation :**
- CPU : 20-40% pendant scraping
- RAM : 300-800 MB
- Bande passante : ~10-50 MB par 100 sites

## ğŸ› DÃ©pannage VPS

### Worker ne dÃ©marre pas

```bash
# VÃ©rifier les logs
sudo journalctl -u scraper-worker -n 50

# Tester manuellement
cd /home/user/scrapeur-site-web
source venv/bin/activate
python worker.py
```

### Permissions refusÃ©es

```bash
# Donner les bonnes permissions
sudo chown -R votre-user:votre-user /home/user/scrapeur-site-web
chmod -R 755 /home/user/scrapeur-site-web
```

### Python introuvable

```bash
# VÃ©rifier Python
which python3
python3 --version

# RecrÃ©er venv si besoin
rm -rf venv
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## ğŸ“ Support

En cas de problÃ¨me :
1. VÃ©rifier les logs : `tail -f logs/worker.log`
2. VÃ©rifier le service : `sudo systemctl status scraper-worker`
3. Tester manuellement : `python worker.py`

