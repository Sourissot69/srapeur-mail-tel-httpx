================================================================================
üöÄ BIENVENUE - SCRAPER EMAILS ET RESEAUX SOCIAUX
================================================================================

Vous venez de cloner le projet ou de l'installer sur un VPS.
Voici comment d√©marrer rapidement !


================================================================================
üìñ √âTAPE 1 : LIRE LA DOCUMENTATION
================================================================================

D√âBUTANT ? ‚Üí Lire : GUIDE_UTILISATION.txt
COMMANDES RAPIDES ? ‚Üí Lire : COMMANDES_RAPIDES.txt
D√âPLOIEMENT VPS ? ‚Üí Lire : DEPLOIEMENT_VPS.md
PUBLICATION GITHUB ? ‚Üí Lire : GITHUB_SETUP.md


================================================================================
‚ö° D√âMARRAGE RAPIDE (5 MINUTES)
================================================================================

1. INSTALLER LES D√âPENDANCES
   ------------------------
   pip install -r requirements.txt


2. D√âMARRER LE WORKER (Terminal 1)
   --------------------------------
   python worker.py

   Vous verrez :
   ################################
   WORKER DEMARRE
   ################################
   En attente de jobs...


3. AJOUTER UN JOB (Terminal 2)
   ----------------------------
   python add_job.py votre_fichier.csv --priority 1 --user "Votre Nom"

   Le worker commencera automatiquement le scraping !


4. CONSULTER L'√âTAT
   -----------------
   python monitor.py


5. R√âCUP√âRER LES R√âSULTATS
   ------------------------
   Les r√©sultats sont dans : results/scraping_*.json


================================================================================
üìä CE QUE FAIT LE SCRAPER
================================================================================

Pour chaque site dans votre CSV, le scraper :

  ‚úÖ Visite la page d'accueil
  ‚úÖ Cherche les pages Contact, Mentions l√©gales, CGV, Politique confidentialit√©
  ‚úÖ Extrait les EMAILS (filtr√©s intelligemment)
  ‚úÖ Extrait les R√âSEAUX SOCIAUX (Facebook, Instagram, LinkedIn, etc.)
  ‚úÖ Sauvegarde tout dans un JSON simplifi√©


EMAILS GARD√âS :
  - Emails du domaine du site (ex: contact@example.com)
  - Emails publics (gmail, hotmail, yahoo, outlook, etc.)


PAGES VISIT√âES :
  - / (accueil)
  - /contact, /contactez-nous
  - /mentions-legales
  - /cgv, /conditions-generales-vente
  - /cgu, /conditions-generales-utilisation
  - /politique-confidentialite, /privacy-policy
  - /rgpd, /donnees-personnelles
  - /about, /a-propos


================================================================================
üìÅ FORMAT CSV REQUIS
================================================================================

Votre CSV doit contenir au minimum la colonne : "Site Web"

Colonnes reconnues (optionnelles) :
  - Nom
  - Type
  - T√©l√©phone Principal
  - Ville
  - Note

Voir : example.csv


================================================================================
‚è±Ô∏è TEMPS DE TRAITEMENT
================================================================================

Performance moyenne : ~2.5 secondes par site

Exemples :
  - 10 sites   = 25 secondes
  - 50 sites   = 2 minutes
  - 100 sites  = 4 minutes
  - 500 sites  = 20 minutes
  - 1000 sites = 40 minutes


================================================================================
üéØ EXEMPLES DE R√âSULTATS
================================================================================

Fichier JSON g√©n√©r√© (simplifi√©) :

[
  {
    "id": 1,
    "url": "https://example.com",
    "nom": "Nom du site",
    "nb_emails": 2,
    "emails": [
      "contact@example.com",
      "info@example.com"
    ],
    "nb_reseaux_sociaux": 3,
    "reseaux_sociaux": {
      "facebook": ["https://facebook.com/example"],
      "instagram": ["https://instagram.com/example"],
      "linkedin": ["https://linkedin.com/company/example"]
    }
  }
]


================================================================================
üîß CONFIGURATION
================================================================================

Le fichier config.py est d√©j√† OPTIMIS√â pour :
  - Vitesse rapide (~2.5s/site)
  - S√©curit√© (√©viter bannissement IP)

Si vous voulez MODIFIER :
  - DELAY_BETWEEN_REQUESTS : D√©lai entre requ√™tes (0.3s par d√©faut)
  - MAX_CONCURRENT_SITES : Sites en parall√®le (10 par d√©faut)
  - MAX_PAGES_PER_SITE : Pages visit√©es par site (7 par d√©faut)


================================================================================
‚ùì BESOIN D'AIDE ?
================================================================================

1. Lire GUIDE_UTILISATION.txt
2. Consulter COMMANDES_RAPIDES.txt
3. V√©rifier les logs : logs/worker.log
4. GitHub Issues (si repo public)


================================================================================
‚úÖ VOUS √äTES PR√äT !
================================================================================

Commencez par : python worker.py

Puis dans un autre terminal : python add_job.py votre_fichier.csv

Bon scraping ! üéâ

