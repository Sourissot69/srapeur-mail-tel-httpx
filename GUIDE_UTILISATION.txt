================================================================================
GUIDE D'UTILISATION - SCRAPER AVEC SYST√àME DE QUEUE
================================================================================

üìã FICHIERS ESSENTIELS DU PROJET
================================================================================

CONFIGURATION :
  - config.py           Configuration (timeouts, user agents, fournisseurs emails)
  - requirements.txt    D√©pendances Python

SCRAPER :
  - scraper.py          ‚≠ê Scraper principal (classe WebScraper)
  - extractors.py       Extraction emails et r√©seaux sociaux
  - utils.py            Fonctions utilitaires

SYST√àME DE QUEUE :
  - add_job.py          ‚≠ê Ajouter un job √† la queue
  - worker.py           ‚≠ê Worker qui traite les jobs
  - monitor.py          ‚≠ê Consulter l'√©tat de la queue

LEGACY (optionnel) :
  - run_scraper.py      Lancer scraping direct sans queue


================================================================================
üöÄ UTILISATION - SC√âNARIO MULTI-UTILISATEURS
================================================================================

√âTAPE 1 : D√âMARRER LE WORKER (1 fois, √† laisser tourner)
------------------------------------------------------------------------
Terminal d√©di√© :

    python worker.py

Le worker affiche :
    ################################
    WORKER DEMARRE
    ################################
    En attente de jobs...


√âTAPE 2 : AJOUTER DES JOBS (depuis n'importe quel terminal)
------------------------------------------------------------------------

User 1 :
    python add_job.py mon-fichier-1.csv --priority 1 --user "Alice"

User 2 :
    python add_job.py mon-fichier-2.csv --priority 2 --user "Bob"

User 3 :
    python add_job.py mon-fichier-3.csv --priority 1 --user "Charlie"


ORDRE DE TRAITEMENT :
    1. mon-fichier-1.csv (priorit√© 1, arriv√© en premier)
    2. mon-fichier-3.csv (priorit√© 1, arriv√© en deuxi√®me)
    3. mon-fichier-2.csv (priorit√© 2)


√âTAPE 3 : CONSULTER L'√âTAT
------------------------------------------------------------------------
N'importe quel terminal :

    python monitor.py

Affiche :
    - Nombre de jobs en attente / en cours / termin√©s
    - Job en cours avec dur√©e
    - Liste des jobs en attente
    - Derniers jobs compl√©t√©s avec stats


================================================================================
üìä FORMAT DES R√âSULTATS
================================================================================

Fichier sauvegard√© dans :
    results/scraping_nom-du-csv_TIMESTAMP.json

Format JSON :
    [
      {
        "id": 1,
        "url": "https://example.com",
        "nom": "Nom du site",
        "nb_emails": 2,
        "emails": ["contact@example.com", "info@example.com"],
        "nb_reseaux_sociaux": 3,
        "reseaux_sociaux": {
          "facebook": ["https://facebook.com/example"],
          "instagram": ["https://instagram.com/example"],
          "linkedin": ["https://linkedin.com/company/example"]
        }
      }
    ]


================================================================================
üìß FILTRAGE DES EMAILS (Important !)
================================================================================

Le scraper garde UNIQUEMENT :
  1. Emails du domaine du site (ex: contact@example.com pour example.com)
  2. Emails de fournisseurs publics (gmail, hotmail, yahoo, outlook, etc.)

Cela √©vite les faux positifs et emails non pertinents.


================================================================================
‚è±Ô∏è PERFORMANCES
================================================================================

Temps moyen : ~2.5 secondes par site

Exemples :
  - 12 sites  = ~30 secondes
  - 50 sites  = ~2 minutes
  - 100 sites = ~4 minutes
  - 500 sites = ~20 minutes


================================================================================
üîß CONFIGURATION (config.py)
================================================================================

Param√®tres optimis√©s pour VITESSE + S√âCURIT√â :

  MAX_CONCURRENT_SITES = 10      # 10 sites en parall√®le
  MAX_PAGES_PER_SITE = 7         # 7 pages max par site
  TIMEOUT = 10                   # 10s timeout par requ√™te
  DELAY_BETWEEN_REQUESTS = 0.3   # 0.3s entre requ√™tes
  SITE_TIMEOUT = 30              # 30s timeout global par site

Si vous voulez PLUS DE S√âCURIT√â (√©viter bannissement) :
  - Augmenter DELAY_BETWEEN_REQUESTS √† 1.0 ou 2.0
  - R√©duire MAX_CONCURRENT_SITES √† 5


================================================================================
üõ°Ô∏è S√âCURIT√â
================================================================================

‚úÖ SAFE :
  - 1 seul worker ‚Üí pas de surcharge
  - D√©lais entre requ√™tes ‚Üí respectueux
  - User-Agents vari√©s ‚Üí pas de d√©tection bot basique
  - Filtrage emails ‚Üí conforme RGPD

‚ö†Ô∏è RISQUES SI MAL UTILIS√â :
  - Scraper 1000+ sites/jour ‚Üí risque de bannissement IP
  - Sites avec protection anti-bot forte ‚Üí √©chec
  - Utilisation commerciale sans accord ‚Üí ill√©gal


================================================================================
üìù EXEMPLES DE COMMANDES
================================================================================

# Ajouter job haute priorit√©
python add_job.py avocats-paris.csv --priority 1 --user "Direction"

# Ajouter job normale
python add_job.py pharmacies-lyon.csv --priority 5 --user "Marketing"

# Ajouter job basse priorit√©
python add_job.py test.csv --priority 10 --user "Dev"

# Voir l'√©tat
python monitor.py

# D√©marrer le worker
python worker.py


================================================================================
üóÇÔ∏è STRUCTURE DU PROJET
================================================================================

scrapeur-de-site-pour-google-map-v2/
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ extractors.py
‚îú‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ scraper.py              ‚≠ê Scraper principal
‚îú‚îÄ‚îÄ add_job.py              ‚≠ê Ajouter job
‚îú‚îÄ‚îÄ worker.py               ‚≠ê Worker
‚îú‚îÄ‚îÄ monitor.py              ‚≠ê Monitoring
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ queue/
‚îÇ   ‚îú‚îÄ‚îÄ pending/           Jobs en attente
‚îÇ   ‚îú‚îÄ‚îÄ processing/        Jobs en cours
‚îÇ   ‚îî‚îÄ‚îÄ completed/         Jobs termin√©s
‚îî‚îÄ‚îÄ results/               R√©sultats JSON


================================================================================
‚úÖ AVANTAGES DE CE SYST√àME
================================================================================

‚úÖ Multi-utilisateurs : Plusieurs personnes peuvent ajouter des jobs
‚úÖ Gestion priorit√©s : Jobs urgents trait√©s en premier
‚úÖ S√©curis√© : 1 seul worker = pas de risque de ban IP
‚úÖ Robuste : Si worker crash, job remis en pending
‚úÖ Simple : Pas de base de donn√©es, juste des fichiers JSON
‚úÖ √âvolutif : Peut migrer vers Redis plus tard si besoin


================================================================================
FIN DU GUIDE
================================================================================

